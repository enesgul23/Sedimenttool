% GRELM - Generalized Regularized Extreme Learning Machine Class
%   Train and Predict a SLFN based on Generalized Regularized Extreme Learning Machine
%
%   This code was implemented based on the following paper:
%
%   [1] Fernando Kentaro Inaba, Evandro Ottoni Teatini Salles, Sylvain
%       Perron, Gilles Caporossi, DGR-ELMâ€“Distributed Generalized
%       Regularized ELM for classification, Neurocomputing, Volume 275, 2018,
%       Pages 1522-1530, ISSN 0925-2312,
%       https://doi.org/10.1016/j.neucom.2017.09.090.
%       (http://www.sciencedirect.com/science/article/pii/S0925231217316193)
%
%   Optimization Problem
%       minimize C/2*|| HB - Y ||_F^2 + \alpha || B ||_{2,1} + (1-\alpha)/2 || B ||_F^2
%
%   Attributes:
%       Attributes between *.* must be informed.
%       GRELM objects must be created using name-value pair arguments (see the Usage Example).
%
%         *numberOfInputNeurons*:   Number of neurons in the input layer.
%                Accepted Values:   Any positive integer.
%
%          numberOfHiddenNeurons:   Initial number of neurons in the hidden layer
%                Accepted Values:   Any positive integer (defaut = 1000).
%
%       regularizationParameter:   Regularization Parameter (defaut = 1000)
%                Accepted Values:   Any positive real number.
%
%                          alpha:   Regularization Parameter (defaut = 0.5)
%                Accepted Values:   Any positive real number between [0,1].
%                                   0: Frob. | 1: L21 | (0,1): mix Frob. and L21
%
%                        maxIter:   Max. number of iteration on ALM (defaut = 20)
%                Accepted Values:   Any positive integer number.
%
%             activationFunction:   Activation funcion for hidden layer
%                Accepted Values:   Function handle (see [1]) or one of these strings:
%                                       'sig':     Sigmoid (default)
%                                       'sin':     Sine
%                                       'hardlim': Hard Limit
%                                       'tribas':  Triangular basis function
%                                       'radbas':  Radial basis function
%
%                           seed:   Seed to generate the pseudo-random values.
%                                   This attribute is for reproducible research.
%                Accepted Values:   RandStream object or a integer seed for RandStream.
%
%       Attributes generated by the code:
%
%                    inputWeight:   Weight matrix that connects the input
%                                   layer to the hidden layer
%
%            biasOfHiddenNeurons:   Bias of hidden units
%
%                   outputWeight:   Weight matrix that connects the hidden
%                                   layer to the output layer
%
%   Methods:
%
%       obj = GRELM(varargin):       Creates GRELM objects. varargin should be in
%                                    pairs. Look attributes
%
%       obj = obj.train(X,Y):        Method for training. X is the input of size N x n,
%                                    where N is (# of samples) and n is the (# of features).
%                                    Y is the output of size N x m, where m is (# of multiple outputs)
%
%       Yhat = obj.predict(X):       Predicts the output for X.
%
%   Usage Example:
%
%       load iris_dataset.mat
%       X    = irisInputs';
%       Y    = irisTargets';
%       grelm  = GRELM('numberOfInputNeurons', 4, 'numberOfHiddenNeurons', 100);
%       grelm  = grelm.train(X, Y);
%       Yhat = grelm.predict(X)

%   License:
%
%   Permission to use, copy, or modify this software and its documentation
%   for educational and research purposes only and without fee is here
%   granted, provided that this copyright notice and the original authors'
%   names appear on all copies and supporting documentation. This program
%   shall not be used, rewritten, or adapted as the basis of a commercial
%   software or hardware product without first obtaining permission of the
%   authors. The authors make no representations about the suitability of
%   this software for any purpose. It is provided "as is" without express
%   or implied warranty.
%
%       Federal University of Espirito Santo (UFES), Brazil
%       Computers and Neural Systems Lab. (LabCISNE)
%       Authors:    F. K. Inaba, B. L. S. Silva, D. L. Cosmo
%       email:      labcisne@gmail.com
%       website:    github.com/labcisne/ELMToolbox
%       date:       Jan/2018

classdef GRELM < ELM
    properties
        regularizationParameter = 1000
        maxIter = 20
        alpha = 0.5
    end
    methods
        
        function self = GRELM(varargin)
            self = self@ELM(varargin{:});
        end
        
        function self = train(self, X, Y)
            auxTime = tic;
            tempH = X*self.inputWeight + repmat(self.biasOfHiddenNeurons,size(X,1),1);
            H = self.activationFunction(tempH);
            clear tempH;
            
            [N, Ntil] = size(H);
            m = size(Y,2);
            
            shrinkage = @(B) bsxfun(@times,max(0,1-self.alpha./sqrt(sum(B.^2,2))),B);
            
            % cholesky decomposition
            eta = (((1-self.alpha)+1)/self.regularizationParameter);
            if ( N >= Ntil )
                L = chol( H'*H + eta*speye(Ntil), 'lower' );
            else
                L = chol( speye(N) + 1/eta*(H*H'), 'lower' );
            end
            L = sparse(L);
            LS = sparse(L');
            
            % save a matrix-vector multiply
            Htb = H'*Y;
            
            % ADMM solver
            Z = zeros(Ntil,m);
            U = zeros(Ntil,m);
            for k = 1:self.maxIter
                % B-update
                q = Htb + (1/self.regularizationParameter)*(Z - U); % temporary value
                if( N >= Ntil )
                    B = LS \ (L \ q);
                else
                    B = q/eta - (H'*(LS \ ( L \ (H*q) )))/eta^2;
                end
                Z = shrinkage(B + U); % Z-update
                U = U + (B - Z);  % U-update
            end
            
            idxAct = any(Z,2);
            self.inputWeight = self.inputWeight(:,idxAct);
            self.biasOfHiddenNeurons = self.biasOfHiddenNeurons(idxAct);
            self.outputWeight = Z(idxAct,:);
            self.numberOfHiddenNeurons = sum(idxAct);
            self.trainTime = tic - auxTime;
        end
        
        function Yhat = predict(self, X)
            auxTime = tic;
            tempH = X*self.inputWeight + repmat(self.biasOfHiddenNeurons,size(X,1),1);
            clear X;
            H = self.activationFunction(tempH);
            Yhat = H * self.outputWeight;
            self.lastTestTime = tic - auxTime;
        end
    end
end
